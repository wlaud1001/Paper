{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation is based on https://www.nitarshan.com/bayes-by-backprop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))\n",
    "import dataloader\n",
    "USE_GPU = 1\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = dataloader.load_MNIST(100, 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(object):\n",
    "    def __init__(self, mu, rho):\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "    @property \n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "    def sample(self):\n",
    "        epsilon = torch.distributions.Normal(0,1).sample(self.rho.size()).to(device)\n",
    "        return self.mu + self.sigma* epsilon\n",
    "    #lnN(x|mu,sigma)\n",
    "    def prob(self, input):\n",
    "        p = torch.exp(-((input - self.mu)**2)/(2 * self.sigma ** 2))/(torch.sqrt(2*torch.tensor(math.pi))*self.sigma)\n",
    "        return p\n",
    "\n",
    "    #lnP(w)\n",
    "    def log_prob(self, input):\n",
    "        return torch.log(self.prob(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2020)\n",
      "tensor(-1.6736)\n"
     ]
    }
   ],
   "source": [
    "mu = torch.tensor(3, dtype = torch.float)\n",
    "rho = torch.tensor(2, dtype = torch.float)\n",
    "a = Gaussian(mu, rho)\n",
    "print(a.sample())\n",
    "print(a.log_prob(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scale_Mixture_Prior(object):\n",
    "    def __init__(self, sigma1, sigma2, pi):\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.gaussian1 = torch.distributions.Normal(0,sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0,sigma2)\n",
    "        self.pi = pi\n",
    "    def log_prob(self, input):\n",
    "        prob1 = torch.exp(self.gaussian1.log_prob(input))\n",
    "        prob2 = torch.exp(self.gaussian2.log_prob(input))\n",
    "        return torch.log(self.pi * prob1 + (1-self.pi) * prob2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3])\n"
     ]
    }
   ],
   "source": [
    "a= [3,3]\n",
    "print(torch.tensor(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BayesianLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5, -4))\n",
    "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
    "        \n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features,).uniform_(-0.2, 0.2))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features,).uniform_(-5, -4))\n",
    "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
    "                   \n",
    "        sigma1=torch.exp(torch.tensor(0.0))\n",
    "        sigma2=torch.exp(torch.tensor(-6.0))\n",
    "        pi = 0.5\n",
    "            \n",
    "        self.scale_mixture_prior = Scale_Mixture_Prior(sigma1, sigma2, pi)\n",
    "        \n",
    "        self.log_prior = 0\n",
    "        self.log_posterior = 0\n",
    "        \n",
    "        \n",
    "    def forward(self, input, sampling = False):\n",
    "        if sampling == True:\n",
    "            weight = self.weight.sample()\n",
    "            bias = self.bias.sample()\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        \n",
    "        \n",
    "        self.log_prior = torch.sum(self.scale_mixture_prior.log_prob(weight)) + torch.sum(self.scale_mixture_prior.log_prob(bias))\n",
    "        self.log_posterior = torch.sum(self.weight.log_prob(weight)) + torch.sum(self.bias.log_prob(bias))\n",
    "\n",
    "            \n",
    "        return F.linear(input, weight, bias)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNet(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_unit=400,num_classes=10):\n",
    "        super(BayesianNet, self).__init__()\n",
    "        self.fc1 = BayesianLinear(input_size, hidden_unit)\n",
    "        self.fc2 = BayesianLinear(hidden_unit, hidden_unit)\n",
    "        self.fc3 = BayesianLinear(hidden_unit, num_classes)\n",
    "        \n",
    "        \n",
    "    def get_log_prior(self):\n",
    "        log_prior = self.fc1.log_prior + self.fc2.log_prior + self.fc3.log_prior\n",
    "        #print('log_prior: {:.4f}'.format(self.fc1.log_prior))\n",
    "        return log_prior\n",
    "    \n",
    "    def get_log_posterior(self):\n",
    "        log_posterior = self.fc1.log_posterior + self.fc2.log_posterior + self.fc3.log_posterior\n",
    "        #print('log_posterior: {:.4f}'.format(self.fc1.log_posterior))\n",
    "        return log_posterior\n",
    "    \n",
    "    def forward(self, input, sampling = False):\n",
    "        x = input.view(input.size(0), -1)\n",
    "        x = F.relu(self.fc1(x, sampling))\n",
    "        x = F.relu(self.fc2(x, sampling))\n",
    "        x = F.log_softmax(self.fc3(x, sampling), dim=1)\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self, input, target, batch_num, num_sampling = 3):\n",
    "        \n",
    "        likelihood_loss = 0\n",
    "        complexity_loss = 0\n",
    "        for i in range(num_sampling):\n",
    "            output = self.forward(input, sampling = True)\n",
    "            log_prior = self.get_log_prior()\n",
    "            log_posterior = self.get_log_posterior()\n",
    "            complexity_loss += log_posterior - log_prior\n",
    "            if USE_GPU:\n",
    "                target = target.type(torch.cuda.LongTensor)\n",
    "            else:\n",
    "                target = target.type(torch.LongTensor)\n",
    "                \n",
    "            \n",
    "            likelihood_loss += F.nll_loss(output, target, size_average=False)\n",
    "        \n",
    "        #print(complexity_loss)\n",
    "        \n",
    "        #print('likelihood_loss:{:.4f}'.format(likelihood_loss))\n",
    "        loss = complexity_loss/batch_num + likelihood_loss\n",
    "            \n",
    "            \n",
    "        return loss/num_sampling, output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 4133.0742, likelihood_loss = 398.4716 training_acc = 4.00, lr = 0.0010\n",
      "training_loss: 3329.0732, likelihood_loss = 398.4716 training_acc = 85.00, lr = 0.0010\n",
      "training_loss: 3154.6277, likelihood_loss = 398.4716 training_acc = 89.00, lr = 0.0010\n",
      "valid acc: 95.00\n",
      "training_loss: 2991.3130, likelihood_loss = 398.4716 training_acc = 96.00, lr = 0.0010\n"
     ]
    }
   ],
   "source": [
    "model = BayesianNet()\n",
    "model.to(device)\n",
    "params = [p.device for p in model.parameters() if p.device]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "num_train = 50000\n",
    "num_val = 10000\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    \n",
    "    train_correct, running_num = 0.0, 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        input, target = data\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, output = model.get_loss(input, target, len(train_loader))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        pred = torch.argmax(output, dim = 1)\n",
    "        train_correct += torch.sum(pred == target)\n",
    "        running_num += input.size(0)    \n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr = param_group['lr']\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print('training_loss: {:.4f}, likelihood_loss = {:.4f} training_acc = {:.2f}, lr = {:.4f}'.format(loss, likelihood_loss, (train_correct*100/running_num), lr))\n",
    "            \n",
    "    #scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    correct =0.0\n",
    "    for i, data in enumerate(valid_loader):\n",
    "        input, target = data\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model.forward(input)\n",
    "        pred = torch.argmax(output, dim = 1)\n",
    "        correct += torch.sum(pred == target)\n",
    "    \n",
    "    #correct = (float)correct\n",
    "    #num_val = (float)num_val\n",
    "    print('valid acc: {:.2f}'.format(correct*100/num_val))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [3,3]\n",
    "print(torch.tensor(a, dtype = torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
